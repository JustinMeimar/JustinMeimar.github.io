---
title: HackGPT Edmonton
layout: page
date: 2023-07-27
---

I had a great experience at my first Hackathon a few weeks ago. It was organized by a company called [Place](www.place.com). There was free pizza and coffee. Also -- we won!

The prompt for the Hackathon was to integrate an LLM into a product. Our idea was to add semantic search to a real estate listings frontend. 

Our approach was to embed textual descriptions of property listings as vectors and keep relevant JSON fields as metadata. The Vector database [Pinecone](https://www.pinecone.io/) made this process simple.

We hoped to allow precise retrievals for quantitative attributes like number of bedrooms, bathrooms etc. Where as for qualitative attributes, we could select vectors with the maximum [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to a users text prompt. 

We used techniques like prompt-chaining to generate richer, standardized query templates and a long term memory, ensuring that each interaction context carried over to the next.

As is typical with Hackathons, our project had many rough edges. Retrievals were limited by the richness of the description. Also, many features that would be desireable to capture, for example: wall color, school proximity, east facing window, simply were either abscent from the description or too subtle to be picked up on. 

We did have some ideas to improve the product, however, that would require more work. 

One was to run image-to-text models to extract descriptions from the 
multiple listing photos, then append the outputs to the existing listing description and embed the entire string. This way we can get those subtle "color of the wall paint" type descriptions to be included in the embeddings. There are challenges, however, with getting models to do this because of the lack of a subject in listing photos. Fine tuning an image model for this domain is far out of my depth at the moment.

Another idea was to allow users to upload several images, and then have our system compare these in the embedded photo space against the gallery images. Essentially using images as the intermediate representation rather than natural language. Imagine someone who has Pinterest board with some aesthetic theme, one could upload the board and receive the house that best matches the aesthetic, avoiding the need to articulate desired attributes.  

In the end, we had a solid presentation and tried to emphasize ideas further improvements. I think the latter was a great boost for us, since everyone is so excited about AI applications at the moment. Thanks to my team members for the great experience -- especially my friend Seif for getting me to sign up.

- Justin


